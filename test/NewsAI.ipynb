{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json, os, string\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "top_dir = os.getcwd()\n",
    "article_dir = top_dir+\"\\\\articles\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loads claims DataFrame from json file\n",
    "df_claims = pd.read_json(open(top_dir + \"\\\\train.json\").read())\n",
    "\n",
    "# Add number of claims to claim DataFrame\n",
    "df_claims['num_related_articles']= df_claims['related_articles'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas_profiling.ProfileReport(df_claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_article_in_variable(article_name):\n",
    "    '''Opens an article and stores it in a variable for python to access'''\n",
    "    f=open(article_dir +article_name, \"r\", encoding=\"utf8\")\n",
    "    article = f.read()\n",
    "    f.close()\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_sentence_stats(article):\n",
    "    '''Takes an article and returns stats about the average sentence length, complexity etc.'''\n",
    "    print(sent_tokenize(article))\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_word_stats(article):\n",
    "    '''Takes an article and returns stast about the number of words, average word length, complexity etc.'''\n",
    "                   \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordListToFreqDict(wordlist):\n",
    "    '''Input: list of words\n",
    "    Output: dict of words and frequency\n",
    "    Takes a list of words, counts the frequency of each word, also removes words of length 1 (single characters)'''\n",
    "    wordfreq = [wordlist.count(p) for p in wordlist]\n",
    "    word_freq_dict = dict(zip(wordlist,wordfreq))\n",
    "    word_freq_dict = {k: v for k, v in word_freq_dict.items() if len(k) > 1}\n",
    "    return word_freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_freq_of_article(article):\n",
    "    stop_words = set(stopwords.words( 'english' ))\n",
    "    word_tokens = word_tokenize(article)\n",
    "    lower_case_words = [x.lower() for x in word_tokens]\n",
    "    filtered_article = [w for w in lower_case_words if not w in stop_words]\n",
    "    filtered_article = []\n",
    "    for w in lower_case_words:\n",
    "       if (w not in stop_words) and (w not in string.punctuation):\n",
    "          filtered_article.append(w)\n",
    "\n",
    "    word_freq_dict = wordListToFreqDict(filtered_article)\n",
    "\n",
    "    return word_freq_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start testing below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['15.txt', '2.txt', '4.txt', '8.txt', '90352.txt', '92265.txt']\n"
     ]
    }
   ],
   "source": [
    "article_list = os.listdir(article_dir) \n",
    "print(article_list)\n",
    "\n",
    "article = open_article_in_variable('92265.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'s: 12\n",
      "texas: 10\n",
      "energy: 7\n",
      "land: 7\n",
      "wind: 7\n",
      "oil: 6\n",
      "office: 5\n",
      "gas: 5\n",
      "power: 4\n",
      "n't: 4\n",
      "money: 4\n",
      "come: 4\n",
      "offshore: 3\n",
      "renewable: 3\n",
      "school: 3\n",
      "leases: 3\n",
      "geothermal: 3\n",
      "patterson: 2\n",
      "future: 2\n",
      "green: 2\n",
      "years: 2\n",
      "nation: 2\n",
      "first: 2\n",
      "ever: 2\n",
      "turbines: 2\n",
      "'re: 2\n",
      "real: 2\n",
      "state: 2\n",
      "revenue: 2\n",
      "billion: 2\n",
      "production: 2\n",
      "gulf: 2\n",
      "right: 2\n",
      "commissioner: 2\n",
      "part: 2\n",
      "produced: 2\n",
      "'m: 2\n",
      "make: 2\n",
      "nearly: 1\n",
      "four: 1\n",
      "ago: 1\n",
      "general: 1\n",
      "signed: 1\n",
      "lease: 1\n",
      "since: 1\n",
      "major: 1\n",
      "hurricanes: 1\n",
      "economic: 1\n",
      "storms: 1\n",
      "blown: 1\n",
      "away: 1\n",
      "plans: 1\n",
      "running: 1\n",
      "mean: 1\n",
      "earning: 1\n",
      "drive: 1\n",
      "develop: 1\n",
      "lands: 1\n",
      "means: 1\n",
      "good: 1\n",
      "put: 1\n",
      "12: 1\n",
      "permanent: 1\n",
      "fund: 1\n",
      "3.7: 1\n",
      "mexico: 1\n",
      "schools: 1\n",
      "taxpayer: 1\n",
      "pocket: 1\n",
      "resources: 1\n",
      "someday: 1\n",
      "depleted: 1\n",
      "making: 1\n",
      "secret: 1\n",
      "peaked: 1\n",
      "1970s: 1\n",
      "job: 1\n",
      "figure: 1\n",
      "replace: 1\n",
      "vital: 1\n",
      "stream: 1\n",
      "gone: 1\n",
      "developing: 1\n",
      "solution: 1\n",
      "currently: 1\n",
      "almost: 1\n",
      "quarter-million: 1\n",
      "near-shore: 1\n",
      "acres: 1\n",
      "development: 1\n",
      "regardless: 1\n",
      "delays: 1\n",
      "earned: 1\n",
      "children: 1\n",
      "451,932.89: 1\n",
      "watt: 1\n",
      "course: 1\n",
      "begin: 1\n",
      "spin: 1\n",
      "like: 1\n",
      "kids: 1\n",
      "earn: 1\n",
      "percentage: 1\n",
      "every: 1\n",
      "bit: 1\n",
      "10.3: 1\n",
      "miles: 1\n",
      "coast: 1\n",
      "confident: 1\n",
      "day: 1\n",
      "sooner: 1\n",
      "rather: 1\n",
      "later: 1\n",
      "sustainable: 1\n",
      "sources: 1\n",
      "solar: 1\n",
      "become: 1\n",
      "viable: 1\n",
      "portfolio: 1\n",
      "dollars: 1\n",
      "sense: 1\n",
      "proud: 1\n",
      "say: 1\n",
      "leading: 1\n",
      "way: 1\n",
      "toward: 1\n",
      "remarkable: 1\n",
      "growth: 1\n",
      "protests: 1\n",
      "rallies: 1\n",
      "kind: 1\n",
      "celebrate: 1\n",
      "elected: 1\n",
      "2002: 1\n",
      "serving: 1\n",
      "second: 1\n",
      "term: 1\n"
     ]
    }
   ],
   "source": [
    "for key, value in sorted(get_word_freq_of_article(article).items(), key=lambda item: item[1], reverse = True):\n",
    "    print(\"%s: %s\" % (key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams, FreqDist\n",
    "all_counts = dict()\n",
    "for size in 2, 3, 4, 5:\n",
    "    all_counts[size] = FreqDist(ngrams(filtered_sentence, size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('30', 'hours', '.'), 2),\n",
       " (('fact', 'check', ':'), 1),\n",
       " (('check', ':', 'employers'), 1),\n",
       " ((':', 'employers', 'cutting'), 1),\n",
       " (('employers', 'cutting', 'back'), 1)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_counts\n",
    "\n",
    "all_counts[3].most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('30', 'hours'), 3), (('hours', '.'), 3), (('part', 'time'), 2), (('increase', 'percentage'), 2)]\n"
     ]
    }
   ],
   "source": [
    "print(all_counts[2].most_common(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
