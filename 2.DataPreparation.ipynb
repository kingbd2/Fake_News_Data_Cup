{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import packages and retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.text import Text\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import string, re\n",
    "import pandas_profiling\n",
    "import local_modules.slack as slack\n",
    "import local_modules.DataPreparation as dp\n",
    "\n",
    "from progressbar import Bar, BouncingBar, Counter, ETA, \\\n",
    "    AdaptiveETA, FileTransferSpeed, FormatLabel, Percentage, \\\n",
    "    ProgressBar, ReverseBar, RotatingMarker, \\\n",
    "    SimpleProgress, Timer, UnknownLength\n",
    "pbar = ProgressBar()\n",
    "%store -r article_df article_df_enriched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download NLTK corpora for stemming, tokenization, lemmatization\n",
    "For more information: https://www.nltk.org/book/ch02.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get word count of articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tokenize, Stem, and Lemmatize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106081</td>\n",
       "      <td>Trump Supporter “Kicked Pregnant Muslim Woman ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>129341</td>\n",
       "      <td>UW Facts and Figures – University of Wisconsin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100963</td>\n",
       "      <td>Gun Control Advocates Target Peaceful Switzerl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12200</td>\n",
       "      <td>U.S. and Republic of Korea Conclude New Specia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>128496</td>\n",
       "      <td>Kremlin's persistent claim of “expected chemic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text\n",
       "0  106081  Trump Supporter “Kicked Pregnant Muslim Woman ...\n",
       "1  129341  UW Facts and Figures – University of Wisconsin...\n",
       "2  100963  Gun Control Advocates Target Peaceful Switzerl...\n",
       "3   12200  U.S. and Republic of Korea Conclude New Specia...\n",
       "4  128496  Kremlin's persistent claim of “expected chemic..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = dp.remove_stopwords(article_df.iloc[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.create_append_feature(article_df, 'word_count', dp.get_word_count, 'text')\n",
    "dp.create_append_feature(article_df, 'token_count', dp.get_token_count, 'text')\n",
    "dp.create_append_feature(article_df, 'brevity_score', dp.brevity_score, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.create_append_feature(article_df, 'filtered_text', dp.remove_stopwords, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>token_count</th>\n",
       "      <th>brevity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106081</td>\n",
       "      <td>Trump Supporter “Kicked Pregnant Muslim Woman ...</td>\n",
       "      <td>189</td>\n",
       "      <td>330</td>\n",
       "      <td>0.572727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>129341</td>\n",
       "      <td>UW Facts and Figures – University of Wisconsin...</td>\n",
       "      <td>40</td>\n",
       "      <td>69</td>\n",
       "      <td>0.579710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100963</td>\n",
       "      <td>Gun Control Advocates Target Peaceful Switzerl...</td>\n",
       "      <td>909</td>\n",
       "      <td>1549</td>\n",
       "      <td>0.586830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12200</td>\n",
       "      <td>U.S. and Republic of Korea Conclude New Specia...</td>\n",
       "      <td>173</td>\n",
       "      <td>284</td>\n",
       "      <td>0.609155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>128496</td>\n",
       "      <td>Kremlin's persistent claim of “expected chemic...</td>\n",
       "      <td>351</td>\n",
       "      <td>679</td>\n",
       "      <td>0.516937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text  word_count  \\\n",
       "0  106081  Trump Supporter “Kicked Pregnant Muslim Woman ...         189   \n",
       "1  129341  UW Facts and Figures – University of Wisconsin...          40   \n",
       "2  100963  Gun Control Advocates Target Peaceful Switzerl...         909   \n",
       "3   12200  U.S. and Republic of Korea Conclude New Specia...         173   \n",
       "4  128496  Kremlin's persistent claim of “expected chemic...         351   \n",
       "\n",
       "   token_count  brevity_score  \n",
       "0          330       0.572727  \n",
       "1           69       0.579710  \n",
       "2         1549       0.586830  \n",
       "3          284       0.609155  \n",
       "4          679       0.516937  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generating sentiment data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Using NLTK vader\n",
    "http://www.nltk.org/howto/sentiment.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    neg: Negative\n",
    "    neu: Neutral\n",
    "    pos: Positive\n",
    "    compound: Compound (i.e. aggregated score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.create_append_feature(article_df, 'brevity_score', dp.get_positive_sentiment, 'text')\n",
    "get_positive_sentiment(sid.polarity_scores, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = article_df['filtered_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos = list that gets populated with positive sentiment for each article with stopwords removed.\n",
    "# neg = list that gets populated with negative sentiment for each article with stopwords removed.\n",
    "# neu = list that gets populated with neutral sentiment for each article with stopwords removed.\n",
    "# comp = list that gets populated with compound score of sentiment for each article with stopwords removed.\n",
    "# j = progress_indicator\n",
    "# pbar = progress_bar\n",
    "j = 0\n",
    "pos = []\n",
    "neg = []\n",
    "neu = []\n",
    "comp = []\n",
    "pbar = ProgressBar(widgets=[Percentage(), Bar(), ETA()], maxval=len(article_df)).start()\n",
    "for article in articles:\n",
    "    article_sentence = ' '.join(word for word in article)\n",
    "    ss = sid.polarity_scores(article_sentence)\n",
    "    pos.append(ss['pos'])\n",
    "    neg.append(ss['neg'])\n",
    "    neu.append(ss['neu'])\n",
    "    comp.append(ss['compound'])\n",
    "    pbar.update(i+1)\n",
    "    j += 1\n",
    "    if j%5000 == 0:\n",
    "        slack.SlackNotification('datacup', '%s / %s articles have been analyzed for sentiment.' % (j, len(article_df)))\n",
    "pbar.finish()\n",
    "article_df['pos'] = pos\n",
    "article_df['neg'] = neg\n",
    "article_df['neu'] = neu\n",
    "article_df['compound'] = comp\n",
    "slack.SlackNotification('datacup', 'All sentiment has been analyzed using the NLTK vader sentiment analysis method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_article_profile = article_df_enriched.profile_report(style={'full_width':True})\n",
    "enriched_article_profile.to_file(output_file=\"data_profiles/enriched_article_data_profile.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store article_df_enriched for loading in Model Development\n",
    "article_df_enriched = article_df\n",
    "%store article_df_enriched "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training data preparation\n",
    "\n",
    "Here we will summarize the article data for each claim, building the training data for model development. \n",
    "Summary statistics include mean, variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/train.json\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "train_df = pd.DataFrame.from_records(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "mean_pos = []\n",
    "mean_neg = []\n",
    "mean_neu = []\n",
    "mean_comp = []\n",
    "mean_brevity = []\n",
    "var_pos = []\n",
    "var_neg = []\n",
    "var_neu = []\n",
    "var_comp = []\n",
    "var_brevity = []\n",
    "for i, (claim) in enumerate(zip(train_df.related_articles)):\n",
    "    brevity = np.zeros([len(claim[0]), 1])\n",
    "    pos = np.zeros([len(claim[0]), 1])\n",
    "    neg = np.zeros([len(claim[0]), 1])\n",
    "    neu = np.zeros([len(claim[0]), 1])\n",
    "    comp = np.zeros([len(claim[0]), 1])\n",
    "    for k, article_id in enumerate(claim[0]):\n",
    "        target = article_df_enriched.loc[article_df_enriched['id'] == article_id]\n",
    "        if target.empty:\n",
    "            continue\n",
    "        brevity[k-1, 0] = target['brevity_score'].values\n",
    "        pos[k-1, 0] = target['pos'].values  \n",
    "        neg[k-1, 0] = target['neg'].values  \n",
    "        neu[k-1, 0] = target['neu'].values\n",
    "        comp[k-1, 0] = target['compound'].values    \n",
    "    mean_pos.append(np.mean(pos))\n",
    "    mean_neg.append(np.mean(neg))\n",
    "    mean_neu.append(np.mean(neu))\n",
    "    mean_comp.append(np.mean(comp))\n",
    "    mean_brevity.append(np.mean(brevity))\n",
    "    var_pos.append(np.var(pos))\n",
    "    var_neg.append(np.var(neg))\n",
    "    var_neu.append(np.var(neu))\n",
    "    var_comp.append(np.var(comp))\n",
    "    var_brevity.append(np.var(brevity))\n",
    "    j += 1\n",
    "    if j%5000 == 0:\n",
    "        slack.SlackNotification('BK_slackbot', '%s / %s claims data have been populated.' % (j, len(train_df)))\n",
    "#     if j == 2:\n",
    "#         break\n",
    "\n",
    "train_df['mean_pos'] = mean_pos\n",
    "train_df['mean_neg'] = mean_neg\n",
    "train_df['mean_neu'] = mean_neu\n",
    "train_df['mean_comp'] = mean_comp\n",
    "train_df['mean_brevity'] = mean_brevity\n",
    "\n",
    "train_df['var_pos'] = var_pos\n",
    "train_df['var_neg'] = var_neg\n",
    "train_df['var_neu'] = var_neu\n",
    "train_df['var_comp'] = var_comp\n",
    "train_df['var_brevity'] = var_brevity\n",
    "\n",
    "slack.SlackNotification('BK_slackbot', 'All claims data have been populated.')\n",
    "\n",
    "# def find_articles():\n",
    "#     df.loc[df['column_name'] == some_value]\n",
    "\n",
    "# def create_summary_stats(item):\n",
    "#     df.loc[df['column_name'] == some_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile training data\n",
    "profile_train_df = train_df.profile_report(style={'full_width':True})\n",
    "profile_train_df.to_file(output_file=\"data_profiles/training_data_profile.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fill out empty data for claimants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['claimant'].replace('', 'anon', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Split data into labels and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels are the values we want to predict\n",
    "labels = train_df['label']\n",
    "features = train_df \\\n",
    "    .drop('claim', axis = 1) \\\n",
    "    .drop('label', axis = 1) \\\n",
    "    .drop('related_articles', axis = 1) \\\n",
    "    .drop('id', axis = 1) \\\n",
    "    .drop('date', axis = 1) \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Standardization of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to encode claimant? High dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_no_claimant = features.drop('claimant', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing# Get column names first\n",
    "names = features_no_claimant.columns# Create the Scaler object\n",
    "scaler = preprocessing.StandardScaler()# Fit your data on the scaler object\n",
    "scaled_features = scaler.fit_transform(features_no_claimant)\n",
    "scaled_features = pd.DataFrame(scaled_features, columns=names)\n",
    "\n",
    "# scaled_labels = scaler.fit_transform(labels.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split # Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(scaled_features, labels, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure splitting was done right\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data for loading in Model Development\n",
    "%store train_features   \n",
    "%store test_features\n",
    "%store train_labels\n",
    "%store test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rough Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "lemma = set([wnl.lemmatize(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(set(tokens))[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tags = nltk.pos_tag(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fake_News_Data_Cup",
   "language": "python",
   "name": "fake_news_data_cup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
